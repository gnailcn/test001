{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hhXsxShx8EiW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel,BertPreTrainedModel,AutoTokenizer\n",
    "import os\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2Ro2stNM-OgA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 256\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 1\n",
    "total_train_epochs = 50\n",
    "\n",
    "output_dir = r'./outputs/'\n",
    "def get_data_dir():\n",
    "    return r'./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8MPOpQid-Y2C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for NER.\"\"\"\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          guid: Unique id for the example(a sentence or a pair of sentences).\n",
    "          words: list of words of sentence\n",
    "          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        # list of words of the sentence,example: [EU, rejects, German, call, to, boycott, British, lamb .]\n",
    "        self.words = words\n",
    "        # list of label sequence of the sentence,like: [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]\n",
    "        self.labels = labels\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\n",
    "    result of convert_examples_to_features(InputExample)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.predict_mask = predict_mask\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"\n",
    "        Reads a BIO data.\n",
    "        \"\"\"\n",
    "        with open(input_file) as f:\n",
    "            # out_lines = []\n",
    "            out_lists = []\n",
    "            entries = f.read().strip().split(\"\\n\\n\")\n",
    "            for entry in entries:\n",
    "                words = []\n",
    "                ner_labels = []\n",
    "                pos_tags = []\n",
    "                bio_pos_tags = []\n",
    "                for line in entry.splitlines():\n",
    "                    pieces = line.strip().split()\n",
    "                    if len(pieces) < 1:\n",
    "                        continue\n",
    "                    word = pieces[0]\n",
    "                    # if word == \"-DOCSTART-\" or word == '':\n",
    "                    #     continue\n",
    "                    words.append(word)\n",
    "                    #pos_tags.append(pieces[1])\n",
    "                    #bio_pos_tags.append(pieces[2])\n",
    "                    ner_labels.append(pieces[-1])\n",
    "                # sentence = ' '.join(words)\n",
    "                # ner_seq = ' '.join(ner_labels)\n",
    "                # pos_tag_seq = ' '.join(pos_tags)\n",
    "                # bio_pos_tag_seq = ' '.join(bio_pos_tags)\n",
    "                # out_lines.append([sentence, pos_tag_seq, bio_pos_tag_seq, ner_seq])\n",
    "                # out_lines.append([sentence, ner_seq])\n",
    "                out_lists.append([words,pos_tags,bio_pos_tags,ner_labels])\n",
    "        return out_lists\n",
    "    \n",
    "class SUB_DataProcessor(DataProcessor):\n",
    "    '''\n",
    "    CTI\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._label_types = self.getLabels()\n",
    "        self._num_labels = len(self._label_types)\n",
    "        self._label_map = {label: i for i,\n",
    "                           label in enumerate(self._label_types)}\n",
    "    def getLabels(self):\n",
    "        path = os.path.join(data_dir, \"train.txt\")\n",
    "        labels = ['X', '[CLS]', '[SEP]']\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            # 将每一行的标签加入到labels中\n",
    "            for line in lines:\n",
    "                if line != '\\n':\n",
    "                    labels.append(line.strip().split()[-1])\n",
    "        # 去重\n",
    "        labels = list(set(labels))\n",
    "        # 返回\n",
    "        return labels\n",
    "\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")))\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"valid.txt\")))\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")))\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._label_types\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self.get_num_labels\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return self._label_map\n",
    "\n",
    "    def get_start_label_id(self):\n",
    "        return self._label_map['[CLS]']\n",
    "\n",
    "    def get_stop_label_id(self):\n",
    "        return self._label_map['[SEP]']\n",
    "\n",
    "    def _create_examples(self, all_lists):\n",
    "        examples = []\n",
    "        for (i, one_lists) in enumerate(all_lists):\n",
    "            guid = i\n",
    "            words = one_lists[0]\n",
    "            labels = one_lists[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def _create_examples2(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = i\n",
    "            text = line[0]\n",
    "            ner_label = line[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text, labels_a=ner_label))\n",
    "        return examples\n",
    "\n",
    "    \n",
    "    \n",
    "class DNRTI_DataProcessor(DataProcessor):\n",
    "    '''\n",
    "    DNRTI_-2003\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._label_types =  [ 'X', '[CLS]', '[SEP]', 'O', 'B-Area', 'B-Exp', 'B-Features', 'B-HackOrg', 'B-Idus', 'B-OffAct','B-Org', 'B-Purp', 'B-SamFile','B-SecTeam','B-Time','B-Tool','B-Way','I-Area','I-Exp','I-Features','I-HackOrg','I-Idus','I-OffAct','I-Org','I-Purp','I-SamFile','I-SecTeam','I-Time','I-Tool','I-Way']\n",
    "        self._num_labels = len(self._label_types)\n",
    "        self._label_map = {label: i for i,\n",
    "                           label in enumerate(self._label_types)}\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")))\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"valid.txt\")))\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")))\n",
    "    def get_predict_examples(self, data_dir,predict_string):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    def get_labels(self):\n",
    "        return self._label_types\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self.get_num_labels\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return self._label_map\n",
    "\n",
    "    def get_start_label_id(self):\n",
    "        return self._label_map['[CLS]']\n",
    "\n",
    "    def get_stop_label_id(self):\n",
    "        return self._label_map['[SEP]']\n",
    "\n",
    "    def _create_examples(self, all_lists):\n",
    "        examples = []\n",
    "        for (i, one_lists) in enumerate(all_lists):\n",
    "            guid = i\n",
    "            words = one_lists[0]\n",
    "            labels = one_lists[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def _create_examples2(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = i\n",
    "            text = line[0]\n",
    "            ner_label = line[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text, labels_a=ner_label))\n",
    "        return examples\n",
    "    \n",
    "def example2feature(example, tokenizer, label_map, max_seq_length):\n",
    "\n",
    "    add_label = 'X'\n",
    "    # tokenize_count = []\n",
    "    tokens = ['[CLS]']\n",
    "    predict_mask = [0]\n",
    "    label_ids = [label_map['[CLS]']]\n",
    "    for i, w in enumerate(example.words):\n",
    "        # use bertTokenizer to split words\n",
    "        # 1996-08-22 => 1996 - 08 - 22\n",
    "        # sheepmeat => sheep ##me ##at\n",
    "        sub_words = tokenizer.tokenize(w)\n",
    "        if not sub_words:\n",
    "            sub_words = ['[UNK]']\n",
    "        # tokenize_count.append(len(sub_words))\n",
    "        tokens.extend(sub_words)\n",
    "        for j in range(len(sub_words)):\n",
    "            if j == 0:\n",
    "                predict_mask.append(1)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "            else:\n",
    "                # '##xxx' -> 'X' (see bert paper)\n",
    "                predict_mask.append(0)\n",
    "                label_ids.append(label_map[add_label])\n",
    "\n",
    "    # truncate\n",
    "    if len(tokens) > max_seq_length - 1:\n",
    "        # print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n",
    "        tokens = tokens[0:(max_seq_length - 1)]\n",
    "        predict_mask = predict_mask[0:(max_seq_length - 1)]\n",
    "        label_ids = label_ids[0:(max_seq_length - 1)]\n",
    "    tokens.append('[SEP]')\n",
    "    predict_mask.append(0)\n",
    "    label_ids.append(label_map['[SEP]'])\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    feat=InputFeatures(\n",
    "                # guid=example.guid,\n",
    "                # tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                predict_mask=predict_mask,\n",
    "                label_ids=label_ids)\n",
    "\n",
    "    return feat\n",
    "\n",
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n",
    "        self.examples=examples\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label_map=label_map\n",
    "        self.max_seq_length=max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat=example2feature(self.examples[idx], self.tokenizer, self.label_map, max_seq_length)\n",
    "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n",
    "\n",
    "    @classmethod\n",
    "    def pad(cls, batch):\n",
    "\n",
    "        seqlen_list = [len(sample[0]) for sample in batch]\n",
    "        maxlen = np.array(seqlen_list).max()\n",
    "\n",
    "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
    "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
    "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
    "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
    "        predict_mask_list = torch.ByteTensor(f(3, maxlen))\n",
    "        label_ids_list = torch.LongTensor(f(4, maxlen))\n",
    "\n",
    "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUrvYXiR-vzz",
    "outputId": "6cabe698-7ce0-4f91-ef3c-71009171ea1a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8676\n",
      "  Batch size = 32\n",
      "  Num steps = 13556\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(get_data_dir(), 'datasets/CTI-reports/')\n",
    "Data_Processor = SUB_DataProcessor()\n",
    "label_list = Data_Processor.get_labels()\n",
    "label_map = Data_Processor.get_label_map()\n",
    "train_examples = Data_Processor.get_train_examples(data_dir)\n",
    "dev_examples = Data_Processor.get_dev_examples(data_dir)\n",
    "test_examples = Data_Processor.get_test_examples(data_dir)\n",
    "total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"% len(train_examples))\n",
    "print(\"  Batch size = %d\"% batch_size)\n",
    "print(\"  Num steps = %d\"% total_train_steps)\n",
    "\n",
    "\n",
    "bert_model_scale = 'xlnet-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_scale, do_lower_case=True)\n",
    "\n",
    "\n",
    "train_dataset = NerDataset(train_examples,tokenizer,label_map,max_seq_length)\n",
    "dev_dataset = NerDataset(dev_examples,tokenizer,label_map,max_seq_length)\n",
    "test_dataset = NerDataset(test_examples,tokenizer,label_map,max_seq_length)\n",
    "\n",
    "train_dataloader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "dev_dataloader = data.DataLoader(dataset=dev_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2CleaZ1s-5cK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2XWZYe9d_grY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    '''\n",
    "    0,1,2,3 are [CLS],[SEP],[X],O\n",
    "    '''\n",
    "    ignore_id = 3\n",
    "\n",
    "    # 计算相关值\n",
    "    num_proposed = (y_pred > ignore_id).sum()\n",
    "    num_correct = ((y_true == y_pred) & (y_true > ignore_id)).sum()\n",
    "    num_gold = (y_true > ignore_id).sum()\n",
    "\n",
    "    # 计算 precision\n",
    "    precision = num_correct / num_proposed if num_proposed > 0 else 0.0\n",
    "\n",
    "    # 计算 recall\n",
    "    recall = num_correct / num_gold if num_gold > 0 else 0.0\n",
    "\n",
    "    # 计算 f1-score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def log_sum_exp_1vec(vec):  # shape(1,m)\n",
    "    max_score = vec[0, np.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n",
    "    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n",
    "\n",
    "def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n",
    "    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzaOQgKu_GXE",
    "outputId": "f20e7ee7-9b71-4014-ec1d-580b09c2eec6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlnet to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertModel were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NER_MODEL(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=5)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (hidden2label): Linear(in_features=768, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build NER model\n",
    "\n",
    "class NER_MODEL(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n",
    "        super(NER_MODEL, self).__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.start_label_id = start_label_id\n",
    "        self.stop_label_id = stop_label_id\n",
    "        self.num_labels = num_labels\n",
    "        # self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device=device\n",
    "\n",
    "        # use pretrainded BertModel\n",
    "        self.bert = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        # Maps the output of the bert into label space.\n",
    "        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.num_labels, self.num_labels))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n",
    "        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n",
    "        # so this enforcement is likely unimportant)\n",
    "        self.transitions.data[start_label_id, :] = -10000\n",
    "        self.transitions.data[:, stop_label_id] = -10000\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden2label.weight)\n",
    "        nn.init.constant_(self.hidden2label.bias, 0.0)\n",
    "        # self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        '''\n",
    "        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX\n",
    "        '''\n",
    "\n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n",
    "        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n",
    "        # self.start_label has all of the score. it is log,0 is p=1\n",
    "        log_alpha[:, 0, self.start_label_id] = 0\n",
    "\n",
    "        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        # feats is the probability of emission, feat.shape=(1,tag_size)\n",
    "        for t in range(1, T):\n",
    "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # log_prob of all barX\n",
    "        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n",
    "        return log_prob_all_barX\n",
    "\n",
    "    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n",
    "        '''\n",
    "        sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        '''\n",
    "        bert_seq_out, _ = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask,return_dict=False)\n",
    "        bert_seq_out = self.dropout(bert_seq_out)\n",
    "        bert_feats = self.hidden2label(bert_seq_out)\n",
    "        return bert_feats\n",
    "\n",
    "    def _score_sentence(self, feats, label_ids):\n",
    "        '''\n",
    "        Gives the score of a provided label sequence\n",
    "        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        '''\n",
    "\n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "        batch_transitions = batch_transitions.flatten(1)\n",
    "\n",
    "        score = torch.zeros((feats.shape[0],1)).to(device)\n",
    "        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n",
    "        for t in range(1, T):\n",
    "            score = score + \\\n",
    "                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n",
    "                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        '''\n",
    "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
    "        '''\n",
    "\n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "\n",
    "        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        log_delta[:, 0, self.start_label_id] = 0\n",
    "\n",
    "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
    "        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n",
    "        for t in range(1, T):\n",
    "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
    "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
    "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
    "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # trace back\n",
    "        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # max p(z1:t,all_x|theta)\n",
    "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # choose the state of z_t according the state choosed of z_t+1.\n",
    "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
    "\n",
    "        return max_logLL_allz_allx, path\n",
    "\n",
    "    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "        forward_score = self._forward_alg(bert_feats)\n",
    "        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        gold_score = self._score_sentence(bert_feats, label_ids)\n",
    "        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n",
    "        return torch.mean(forward_score - gold_score)\n",
    "\n",
    "    # this forward is just for predict, not for train\n",
    "    # dont confuse this with _forward_alg above.\n",
    "    def forward(self, input_ids, segment_ids, input_mask):\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, label_seq_ids = self._viterbi_decode(bert_feats)\n",
    "        return score, label_seq_ids\n",
    "\n",
    "\n",
    "start_label_id = Data_Processor.get_start_label_id()\n",
    "stop_label_id = Data_Processor.get_stop_label_id()\n",
    "bert_model = BertModel.from_pretrained('xlnet-base-cased')\n",
    "model = NER_MODEL(bert_model, start_label_id, stop_label_id, len(label_list), max_seq_length, batch_size, device)\n",
    "start_epoch = 0\n",
    "valid_acc_prev = 0\n",
    "valid_f1_prev = 0\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "39GMEKcP_sZv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 超參數\n",
    "\n",
    "\n",
    "lr0_crf_fc = 8e-5\n",
    "learning_rate0 = 5e-5\n",
    "weight_decay_crf_fc = 5e-6 #0.005\n",
    "weight_decay_finetune = 1e-5 #0.01\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "new_param = ['transitions', 'hidden2label.weight', 'hidden2label.bias']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': weight_decay_finetune},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in param_optimizer if n in ('transitions','hidden2label.weight')] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': weight_decay_crf_fc},\n",
    "    {'params': [p for n, p in param_optimizer if n == 'hidden2label.bias'] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "#optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VdLGNEW5_22_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import time\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n",
    "# def evaluate(model, predict_dataloader, batch_size, epoch_th, dataset_name):\n",
    "#     # print(\"***** Running prediction *****\")\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     total=0\n",
    "#     correct=0\n",
    "#     start = time.time()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in predict_dataloader:\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "#             input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "#             _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "#             # _, predicted = torch.max(out_scores, -1)\n",
    "#             valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
    "#             valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
    "#             all_preds.extend(valid_predicted.tolist())\n",
    "#             all_labels.extend(valid_label_ids.tolist())\n",
    "#             # print(len(valid_label_ids),len(valid_predicted),len(valid_label_ids)==len(valid_predicted))\n",
    "#             total += len(valid_label_ids)\n",
    "#             correct += valid_predicted.eq(valid_label_ids).sum().item()\n",
    "\n",
    "#     test_acc = correct/total\n",
    "#     precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n",
    "#     end = time.time()\n",
    "#     print('Epoch:%d, Acc:%.2f, Precision: %.2f, Recall: %.2f, F1: %.2f on %s, Spend:%.3f minutes for evaluation' % (epoch_th, 100.*test_acc, 100.*precision, 100.*recall, 100.*f1, dataset_name,(end-start)/60.0))\n",
    "#     print('--------------------------------------------------------------')\n",
    "#     return test_acc, f1\n",
    "\n",
    "\n",
    "def validModel(model, dataloader):\n",
    "    \"\"\"\n",
    "    验证模型性能，不计算每个实体的单独指标\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "            _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "            valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
    "            valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
    "            all_preds.extend(valid_predicted.tolist())\n",
    "            all_labels.extend(valid_label_ids.tolist())\n",
    "            total += len(valid_label_ids)\n",
    "            correct += valid_predicted.eq(valid_label_ids).sum().item()\n",
    "    \n",
    "    # 计算总体准确率和其他指标\n",
    "    acc = correct / total\n",
    "    precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n",
    "    \n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n",
    "\n",
    "warmup_proportion = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lR1G6D10_47T",
    "outputId": "edae8474-95fb-4476-f4df-773321090150",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Epoch:0 completed, Total training's Loss: 380348836.5469, Spend: 0.910m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108205/2756584519.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
      "/tmp/ipykernel_108205/2756584519.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  valid_label_ids = torch.masked_select(label_ids, predict_mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Acc:62.40, Precision: 62.40, Recall: 62.43, F1: 62.41 on validation set, Spend:0.064 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:1 completed, Total training's Loss: 385108658.0469, Spend: 0.938m\n",
      "Epoch:1, Acc:88.73, Precision: 88.73, Recall: 88.78, F1: 88.75 on validation set, Spend:0.060 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:2 completed, Total training's Loss: 380890928.2656, Spend: 0.891m\n",
      "Epoch:2, Acc:96.27, Precision: 96.27, Recall: 96.32, F1: 96.30 on validation set, Spend:0.064 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:3 completed, Total training's Loss: 373429558.9336, Spend: 0.893m\n",
      "Epoch:3, Acc:96.68, Precision: 96.68, Recall: 96.73, F1: 96.71 on validation set, Spend:0.060 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:4 completed, Total training's Loss: 373927342.6094, Spend: 0.888m\n",
      "Epoch:4, Acc:96.87, Precision: 96.87, Recall: 96.92, F1: 96.90 on validation set, Spend:0.061 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:5 completed, Total training's Loss: 381219630.4531, Spend: 0.900m\n",
      "Epoch:5, Acc:97.08, Precision: 97.08, Recall: 97.12, F1: 97.10 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:6 completed, Total training's Loss: 382654304.9375, Spend: 0.899m\n",
      "Epoch:6, Acc:97.19, Precision: 97.19, Recall: 97.24, F1: 97.21 on validation set, Spend:0.063 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:7 completed, Total training's Loss: 376659378.0078, Spend: 0.903m\n",
      "Epoch:7, Acc:97.09, Precision: 97.09, Recall: 97.14, F1: 97.12 on validation set, Spend:0.063 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:8 completed, Total training's Loss: 368869538.7969, Spend: 0.918m\n",
      "Epoch:8, Acc:97.13, Precision: 97.13, Recall: 97.18, F1: 97.15 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:9 completed, Total training's Loss: 372290726.9219, Spend: 0.934m\n",
      "Epoch:9, Acc:97.03, Precision: 97.04, Recall: 97.08, F1: 97.06 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:10 completed, Total training's Loss: 372605922.4531, Spend: 0.929m\n",
      "Epoch:10, Acc:96.38, Precision: 96.38, Recall: 96.43, F1: 96.40 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:11 completed, Total training's Loss: 389127514.0625, Spend: 0.958m\n",
      "Epoch:11, Acc:97.19, Precision: 97.20, Recall: 97.24, F1: 97.22 on validation set, Spend:0.061 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:12 completed, Total training's Loss: 373728892.1406, Spend: 0.914m\n",
      "Epoch:12, Acc:97.21, Precision: 97.21, Recall: 97.26, F1: 97.24 on validation set, Spend:0.066 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:13 completed, Total training's Loss: 369204643.7969, Spend: 0.910m\n",
      "Epoch:13, Acc:97.23, Precision: 97.23, Recall: 97.28, F1: 97.26 on validation set, Spend:0.069 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:14 completed, Total training's Loss: 386875653.2188, Spend: 0.969m\n",
      "Epoch:14, Acc:97.07, Precision: 97.07, Recall: 97.11, F1: 97.09 on validation set, Spend:0.063 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:15 completed, Total training's Loss: 368299148.3281, Spend: 0.923m\n",
      "Epoch:15, Acc:96.82, Precision: 96.82, Recall: 96.87, F1: 96.85 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:16 completed, Total training's Loss: 378846347.6484, Spend: 0.955m\n",
      "Epoch:16, Acc:96.97, Precision: 96.97, Recall: 97.02, F1: 97.00 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:17 completed, Total training's Loss: 372422789.4531, Spend: 0.930m\n",
      "Epoch:17, Acc:97.26, Precision: 97.26, Recall: 97.31, F1: 97.28 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:18 completed, Total training's Loss: 366370775.0625, Spend: 0.912m\n",
      "Epoch:18, Acc:97.03, Precision: 97.03, Recall: 97.07, F1: 97.05 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:19 completed, Total training's Loss: 365485919.3828, Spend: 0.935m\n",
      "Epoch:19, Acc:97.16, Precision: 97.16, Recall: 97.21, F1: 97.18 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:20 completed, Total training's Loss: 379031351.2969, Spend: 0.926m\n",
      "Epoch:20, Acc:97.17, Precision: 97.18, Recall: 97.22, F1: 97.20 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:21 completed, Total training's Loss: 358944171.9531, Spend: 0.919m\n",
      "Epoch:21, Acc:97.32, Precision: 97.32, Recall: 97.37, F1: 97.34 on validation set, Spend:0.059 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:22 completed, Total training's Loss: 366755664.7812, Spend: 0.931m\n",
      "Epoch:22, Acc:97.32, Precision: 97.32, Recall: 97.36, F1: 97.34 on validation set, Spend:0.061 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:23 completed, Total training's Loss: 367036642.2500, Spend: 0.939m\n",
      "Epoch:23, Acc:97.08, Precision: 97.08, Recall: 97.12, F1: 97.10 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:24 completed, Total training's Loss: 371222848.4062, Spend: 0.936m\n",
      "Epoch:24, Acc:97.27, Precision: 97.27, Recall: 97.31, F1: 97.29 on validation set, Spend:0.061 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:25 completed, Total training's Loss: 366074893.9688, Spend: 0.926m\n",
      "Epoch:25, Acc:97.04, Precision: 97.04, Recall: 97.09, F1: 97.06 on validation set, Spend:0.058 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:26 completed, Total training's Loss: 348935891.0312, Spend: 0.887m\n",
      "Epoch:26, Acc:97.15, Precision: 97.15, Recall: 97.19, F1: 97.17 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:27 completed, Total training's Loss: 358107420.1406, Spend: 0.923m\n",
      "Epoch:27, Acc:97.20, Precision: 97.20, Recall: 97.24, F1: 97.22 on validation set, Spend:0.060 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:28 completed, Total training's Loss: 361643790.8750, Spend: 0.940m\n",
      "Epoch:28, Acc:97.26, Precision: 97.26, Recall: 97.31, F1: 97.28 on validation set, Spend:0.063 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:29 completed, Total training's Loss: 358230234.3594, Spend: 0.927m\n",
      "Epoch:29, Acc:97.26, Precision: 97.26, Recall: 97.30, F1: 97.28 on validation set, Spend:0.063 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:30 completed, Total training's Loss: 361653691.4844, Spend: 0.944m\n",
      "Epoch:30, Acc:97.27, Precision: 97.27, Recall: 97.31, F1: 97.29 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:31 completed, Total training's Loss: 361550418.9375, Spend: 0.930m\n",
      "Epoch:31, Acc:97.24, Precision: 97.24, Recall: 97.28, F1: 97.26 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:32 completed, Total training's Loss: 364123315.2891, Spend: 0.943m\n",
      "Epoch:32, Acc:97.29, Precision: 97.29, Recall: 97.33, F1: 97.31 on validation set, Spend:0.063 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:33 completed, Total training's Loss: 350705210.7344, Spend: 0.902m\n",
      "Epoch:33, Acc:97.21, Precision: 97.21, Recall: 97.25, F1: 97.23 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:34 completed, Total training's Loss: 364706398.9688, Spend: 0.966m\n",
      "Epoch:34, Acc:97.10, Precision: 97.10, Recall: 97.15, F1: 97.12 on validation set, Spend:0.068 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:35 completed, Total training's Loss: 364141998.0312, Spend: 0.978m\n",
      "Epoch:35, Acc:97.20, Precision: 97.20, Recall: 97.24, F1: 97.22 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:36 completed, Total training's Loss: 359045803.1250, Spend: 0.945m\n",
      "Epoch:36, Acc:97.28, Precision: 97.28, Recall: 97.32, F1: 97.30 on validation set, Spend:0.067 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:37 completed, Total training's Loss: 358928939.7188, Spend: 0.940m\n",
      "Epoch:37, Acc:97.29, Precision: 97.29, Recall: 97.33, F1: 97.31 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:38 completed, Total training's Loss: 352926969.4531, Spend: 0.922m\n",
      "Epoch:38, Acc:97.34, Precision: 97.34, Recall: 97.39, F1: 97.36 on validation set, Spend:0.064 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:39 completed, Total training's Loss: 348373298.0938, Spend: 0.903m\n",
      "Epoch:39, Acc:97.33, Precision: 97.33, Recall: 97.37, F1: 97.35 on validation set, Spend:0.064 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:40 completed, Total training's Loss: 347138211.9219, Spend: 0.925m\n",
      "Epoch:40, Acc:97.28, Precision: 97.28, Recall: 97.32, F1: 97.30 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:41 completed, Total training's Loss: 365994893.6094, Spend: 0.967m\n",
      "Epoch:41, Acc:97.36, Precision: 97.36, Recall: 97.41, F1: 97.38 on validation set, Spend:0.069 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:42 completed, Total training's Loss: 356895640.7969, Spend: 0.961m\n",
      "Epoch:42, Acc:97.33, Precision: 97.33, Recall: 97.37, F1: 97.35 on validation set, Spend:0.068 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:43 completed, Total training's Loss: 359651408.3594, Spend: 0.975m\n",
      "Epoch:43, Acc:97.29, Precision: 97.29, Recall: 97.33, F1: 97.31 on validation set, Spend:0.064 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:44 completed, Total training's Loss: 348464456.4766, Spend: 0.966m\n",
      "Epoch:44, Acc:97.27, Precision: 97.27, Recall: 97.31, F1: 97.29 on validation set, Spend:0.065 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:45 completed, Total training's Loss: 351917686.2656, Spend: 0.930m\n",
      "Epoch:45, Acc:97.29, Precision: 97.29, Recall: 97.34, F1: 97.31 on validation set, Spend:0.067 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:46 completed, Total training's Loss: 350784718.7812, Spend: 0.921m\n",
      "Epoch:46, Acc:97.36, Precision: 97.36, Recall: 97.40, F1: 97.38 on validation set, Spend:0.062 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:47 completed, Total training's Loss: 349453293.5234, Spend: 0.907m\n",
      "Epoch:47, Acc:97.31, Precision: 97.31, Recall: 97.35, F1: 97.33 on validation set, Spend:0.067 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:48 completed, Total training's Loss: 351034878.2812, Spend: 0.941m\n",
      "Epoch:48, Acc:97.27, Precision: 97.27, Recall: 97.31, F1: 97.29 on validation set, Spend:0.064 minutes for evaluation\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Epoch:49 completed, Total training's Loss: 348739109.2871, Spend: 0.927m\n",
      "Epoch:49, Acc:97.31, Precision: 97.31, Recall: 97.35, F1: 97.33 on validation set, Spend:0.067 minutes for evaluation\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training Model and evaluation\n",
    "\n",
    "results = []\n",
    "\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    tr_loss = 0\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "\n",
    "        neg_log_likelihood = model.neg_log_likelihood(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            neg_log_likelihood = neg_log_likelihood / gradient_accumulation_steps\n",
    "\n",
    "        neg_log_likelihood.backward()\n",
    "\n",
    "        tr_loss += neg_log_likelihood.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = learning_rate0 * warmup_linear(global_step_th/total_train_steps, warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "\n",
    "    train_time = (time.time() - train_start) / 60.0\n",
    "    print('--------------------------------------------------------------')\n",
    "    print(\"Epoch:{} completed, Total training's Loss: {:.4f}, Spend: {:.3f}m\".format(epoch, tr_loss, train_time))\n",
    "    \n",
    "    # Validation\n",
    "    start = time.time()\n",
    "    valid_acc, valid_precision, valid_recall, valid_f1 = validModel(model, dev_dataloader)\n",
    "    end = time.time()\n",
    "\n",
    "    print('Epoch:%d, Acc:%.2f, Precision: %.2f, Recall: %.2f, F1: %.2f on %s, Spend:%.3f minutes for evaluation' % \n",
    "          (epoch, 100. * valid_acc, 100. * valid_precision, 100. * valid_recall, 100. * valid_f1, 'validation set', (end - start) / 60.0))\n",
    "    print('--------------------------------------------------------------')\n",
    "\n",
    "    # Save a checkpoint\n",
    "    if valid_f1 > valid_f1_prev:\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_acc': valid_acc,\n",
    "            'valid_f1': valid_f1, 'max_seq_length': max_seq_length, 'lower_case': False},\n",
    "                    os.path.join(output_dir, 'xlnet_crf_cti_checkpoint.pt'))\n",
    "        valid_f1_prev = valid_f1\n",
    "    \n",
    "    # Save results\n",
    "    results.append(\"{},{:.4f},{:.4f},{:.4f},{:.4f},{:.4f}\".format(\n",
    "        epoch, tr_loss, valid_acc, valid_precision, valid_recall, valid_f1))\n",
    "\n",
    "# Save results to a .txt file\n",
    "output_file = './outputs/cti_train.txt'\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"Epoch,Training Loss,Validation Acc,Validation Precision,Validation Recall,Validation F1\\n\")\n",
    "    f.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UTqpiNtoBcKu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108205/689590463.py:19: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)  # 有效预测值\n",
      "/tmp/ipykernel_108205/689590463.py:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  valid_label_ids = torch.masked_select(label_ids, predict_mask)  # 有效真实标签\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-Entity Metrics:\n",
      "Entity: [CLS], Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: I-malware.infosteal, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: I-url.normal, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: [SEP], Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: O, Precision: 0.99, Recall: 0.99, F1: 0.99\n",
      "Entity: I-url.cncsvr, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: B-malware.backdoor, Precision: 0.38, Recall: 0.52, F1: 0.44\n",
      "Entity: I-ip.unknown, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: B-url.cncsvr, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: B-hash, Precision: 0.44, Recall: 0.31, F1: 0.36\n",
      "Entity: X, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: B-malware.infosteal, Precision: 0.27, Recall: 0.24, F1: 0.26\n",
      "Entity: I-malware.backdoor, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: I-hash, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: B-malware.ransom, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: I-malware.drop, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: I-malware.unknown, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: B-malware.drop, Precision: 0.43, Recall: 0.04, F1: 0.07\n",
      "Entity: B-ip.unknown, Precision: 0.44, Recall: 0.20, F1: 0.27\n",
      "Entity: B-malware.unknown, Precision: 0.50, Recall: 0.06, F1: 0.11\n",
      "Entity: I-malware.ransom, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Entity: B-url.unknown, Precision: 0.17, Recall: 0.02, F1: 0.04\n",
      "Entity: B-url.normal, Precision: 0.51, Recall: 0.27, F1: 0.36\n",
      "Entity: I-url.unknown, Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Acc:97.41, Precision: 97.41, Recall: 97.46, F1: 97.43 on test, Spend:0.078 minutes for evaluation\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def testModel(model, dataloader, label_list):\n",
    "    \"\"\"\n",
    "    测试模型性能，计算整体指标和每个实体的单独指标\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    label_metrics = {label: {'TP': 0, 'FP': 0, 'FN': 0} for label in label_list}\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "            # 获取模型预测\n",
    "            _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "            valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)  # 有效预测值\n",
    "            valid_label_ids = torch.masked_select(label_ids, predict_mask)  # 有效真实标签\n",
    "            \n",
    "            # 转换为 NumPy 列表\n",
    "            valid_predicted = valid_predicted.cpu().numpy()\n",
    "            valid_label_ids = valid_label_ids.cpu().numpy()\n",
    "\n",
    "            # 更新总体数据\n",
    "            all_preds.extend(valid_predicted.tolist())\n",
    "            all_labels.extend(valid_label_ids.tolist())\n",
    "            total += len(valid_label_ids)\n",
    "            correct += (valid_predicted == valid_label_ids).sum()\n",
    "\n",
    "            # 更新每个标签的 TP、FP、FN\n",
    "            for label_idx, label_name in enumerate(label_list):\n",
    "                preds_label = (valid_predicted == label_idx)\n",
    "                labels_label = (valid_label_ids == label_idx)\n",
    "                label_metrics[label_name]['TP'] += np.sum(preds_label & labels_label)\n",
    "                label_metrics[label_name]['FP'] += np.sum(preds_label & ~labels_label)\n",
    "                label_metrics[label_name]['FN'] += np.sum(~preds_label & labels_label)\n",
    "\n",
    "    # 计算总体指标\n",
    "    test_acc = correct / total\n",
    "    precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n",
    "\n",
    "    # 保存预测和真实标签到文件\n",
    "    output_file = './outputs/cti_test.txt'\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        for true, pred in zip(all_labels, all_preds):\n",
    "            f.write(f\"{true}\\t{pred}\\n\")\n",
    "\n",
    "    \n",
    "    # 打印每个标签的指标\n",
    "    print(\"\\nPer-Entity Metrics:\")\n",
    "    for label, metrics in label_metrics.items():\n",
    "        tp = metrics['TP']\n",
    "        fp = metrics['FP']\n",
    "        fn = metrics['FN']\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_label = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        print(f\"Entity: {label}, Precision: {prec:.2f}, Recall: {rec:.2f}, F1: {f1_label:.2f}\")\n",
    "\n",
    "    end = time.time()\n",
    "    print('Acc:%.2f, Precision: %.2f, Recall: %.2f, F1: %.2f on test, Spend:%.3f minutes for evaluation' % \n",
    "          (100. * test_acc, 100. * precision, 100. * recall, 100. * f1, (end - start) / 60.0))\n",
    "    print('--------------------------------------------------------------')\n",
    "\n",
    "\n",
    "testModel(model, test_dataloader, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
